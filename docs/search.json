[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I am Goran, a graduated mathematician with a PhD in theoretical physics. I am currently working as a datascientist in a German insurance company. This blog contains articles, notebooks and code-snippets about machine learning, physics and Julia."
  },
  {
    "objectID": "posts/Lux-regularisation/lux_dense_regularisation.html",
    "href": "posts/Lux-regularisation/lux_dense_regularisation.html",
    "title": "Regularisation in Lux.jl",
    "section": "",
    "text": "Regularization is a common technique in machine learning used to improve model performance and generalization. It involves adding a penalty term to the loss function, typically to prevent overfitting. Overfitting occurs when a model captures patterns specific to the training data but fails to generalize well to unseen data. Regularization can also help control exploding weights or enforce sparsity, which can lead to more interpretable models. In this notebook, we will demonstrate how to incorporate regularization into model training using Lux.jl.\nLux.jl is a machine learning library built entirely in pure Julia. It is designed for simplicity, flexibility, and high performance. One of its defining features is the strict separation between model parameters and layer structures. This approach is somewhat different from other machine learning libraries. But it provides deeper insight into the inner workings of model training and architecture.\n\n1 Setup\nLet us first install Lux.jl and other required packages …\n\nimport Pkg\nPkg.activate(\".\")\nPkg.add([\"Lux\", \"Random\", \"Printf\", \"Enzyme\", \"Optimisers\"])\n\n… and load them into the notebook.\n\nusing Lux, Random, Printf, Enzyme, Optimisers\n\nBesides Lux we need - Random for generating random numbers - Printf for formatted printing - Enzyme for automatic differentiation of the models and loss functions - Optimisers for the ADAM optimiser\nLets define a simple multi-layer perceptron (MLP) model with a single hidden layer and \\(\\tanh\\) activation function.\n\nmodel = Chain(\n    Dense(2, 4, tanh),\n    Dense(4, 1),\n)\n\n\nChain(\n    layer_1 = Dense(2 =&gt; 4, tanh),      # 12 parameters\n    layer_2 = Dense(4 =&gt; 1),            # 5 parameters\n)         # Total: 17 parameters,\n          #        plus 0 states.\n\n\n\nBesides the separation of model parameters and layer structure, Lux takes randomness very seriously, too. To align with this design philosophy, we make this notebook reproducible by fixing the seed of the random number generator and generating dummy training data.\n\nrng = Random.default_rng()\nRandom.seed!(rng, 42)\n\nX_train = randn(rng, Float32, 2,100)\ny_train = randn(rng, Float32, 1,100)\n\n1×100 Matrix{Float32}:\n -1.48774  -1.55746  -0.013772  0.423002  …  -0.749528  -0.293559  -1.15156\n\n\nAs mentioned, Lux keeps model parameters and layer structures separate. The parameters are set up using a setup method, which returns the parameters ps and the layer states st. In our simple MLP example, the layers don’t have any states. But for layers like batch normalization, you’d see states being used.\n\nps, st = LuxCore.setup(rng, model)\n\n((layer_1 = (weight = Float32[0.353489 -1.8094461; 1.5090263 -1.1654156; -0.83640474 -1.1235346; 1.8991852 1.449335], bias = Float32[0.41897145, -0.069400184, -0.6758513, 0.67588615]), layer_2 = (weight = Float32[0.44184172 0.014959536 0.5158945 0.3650591], bias = Float32[0.24236047])), (layer_1 = NamedTuple(), layer_2 = NamedTuple()))\n\n\nTo evaluate the model, we simply call it like a regular function, passing the model parameters (and the empty states) as function arguments.\n\nmodel(X_train[:,1], ps, st)\n\n(Float32[0.7788858], (layer_1 = NamedTuple(), layer_2 = NamedTuple()))\n\n\n\n\n2 Regularisation\nThere are multiple ways to regularize a model. Some common techniques include \\(l_1\\) and \\(l_2\\) regularization. In this notebook, we will focus on L2 regularization, which is also known as weight decay. It involves adding a penalty term to the loss function that is proportional to the square of the model parameters (weights).\nOn my first attempt to implement \\(l_2\\) regularisation of model parameters I wrote the following custom loss function:\nfunction loss_function(model, ps, st, (x, y))\n    T = eltype(Base.Flatten(first(ps)))\n\n    loss_mse = MSELoss()(model, ps, st, (x,y))[1]\n\n    loss_reg = zero(T)\n    for p in ps\n        loss_reg += sum(abs2, Base.Flatten(p))\n    end\n\n    loss_total = loss_mse + convert(T, 0.001) * loss_reg\n\n    return loss_total, st, NamedTuple()\nend\nIt works fine but isn’t the most extendable solution. The Julia community on Discourse suggested using the WeightDecay function from Optimisers.jl, which does exactly what we need. By chaining it with the MSELoss function, we can train our dummy MLP model with \\(l_2\\) regularization like this:\n\n# code adapted from the Lux documentation https://lux.csail.mit.edu/stable/\nfunction train_model!(model, ps, st, x, y)\n\n    train_state = Lux.Training.TrainState(model, ps, st,\n        # here we chain together the optimiser Adam with \n        # a WeightDecay of 0.001. \n        OptimiserChain(Adam(0.01f0), WeightDecay(0.001)),\n    )\n\n    for iter in 1:1000\n        _, loss, _, train_state = Lux.Training.single_train_step!(\n            AutoEnzyme(),\n            MSELoss(),\n            (x, y), train_state\n        )\n        if iter % 100 == 1 || iter == 1000\n            @printf \"Iteration: %04d \\t Loss: %10.9g\\n\" iter loss\n        end\n    end\n\n    return model, ps, st\nend\n\ntrain_model!(model, ps, st, X_train, y_train)\n\nIteration: 0001      Loss: 1.24611223\nIteration: 0101      Loss: 0.991424084\nIteration: 0201      Loss: 0.959512353\nIteration: 0301      Loss: 0.949317396\nIteration: 0401      Loss: 0.941184938\nIteration: 0501      Loss: 0.932920218\nIteration: 0601      Loss: 0.922357559\nIteration: 0701      Loss: 0.910138071\nIteration: 0801      Loss: 0.898681343\nIteration: 0901      Loss: 0.888456821\nIteration: 1000      Loss: 0.879557967\n\n\n(Chain{@NamedTuple{layer_1::Dense{typeof(tanh), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(2 =&gt; 4, tanh), layer_2 = Dense(4 =&gt; 1)), nothing), (layer_1 = (weight = Float32[5.620661 -4.1725636; 3.3105361 -3.3534684; -1.4295509 -0.7033358; 3.2561698 1.6087654], bias = Float32[0.7682078, 0.48789456, -0.012730246, -0.3011559]), layer_2 = (weight = Float32[1.2095982 -1.1726627 1.2155563 0.9848269], bias = Float32[0.29594985])), (layer_1 = NamedTuple(), layer_2 = NamedTuple()))\n\n\nThat’s it! Just chain together the optimizer Adam with WeightDecay via OptimiserChain and we have \\(l_2\\) regularisation of model weights."
  },
  {
    "objectID": "posts/insurance-claim-frequency/freMTPL2freq.html",
    "href": "posts/insurance-claim-frequency/freMTPL2freq.html",
    "title": "Modelling the frequency of insurance claims",
    "section": "",
    "text": "Predicting future number of claims made by policy holders is at the heart of every insurance company’s business model. In this blogpost, we will show you how to estimate future number of claims using generalized linear models (GLMs) in Julia.\nWe will work with the freMTPL2freq dataset, which contains insurance claims data from French policyholders of car insurances. Our goal is to model the relationship between policyholder characteristics and their expected number of claims per year — the claim frequency.\nFor a fresh perspective on this task, we will use Julia, a high-performance programming language tailored for numerical and scientific computing. Julia stands out by combining the execution speed of low-level languages like C with the intuitive syntax and ease of use typically associated with high-level languages like Python.\nThis blog post is structured into several sections. We will begin by loading the data, followed by exploring and visualizing key statistics from the dataset. Next, we will preprocess the data to prepare it for modeling. Finally, we will fit a generalized linear model to predict the claim frequency."
  },
  {
    "objectID": "posts/insurance-claim-frequency/freMTPL2freq.html#download-the-dataset",
    "href": "posts/insurance-claim-frequency/freMTPL2freq.html#download-the-dataset",
    "title": "Modelling the frequency of insurance claims",
    "section": "1.1 Download the dataset",
    "text": "1.1 Download the dataset\nThe dataset we are using is the freMTPL2freq dataset. The freMTPL2freq dataset contains risk features for 677,991 motor third-party liability policies, observed over one year. It is described in Computational Actuarial Science with R (Arthur Charpentier, CRC, 2018) and is available on OpenML. The following command will automatically download the dataset from OpenML, if it isn’t already available in the current working directory:\n\nfile_path = \"freMTPL2freq.arff\"\nif !isfile(file_path)\n    using Downloads\n    Downloads.download(\"https://www.openml.org/data/download/20649148/freMTPL2freq.arff\", file_path)\nend\n\nThe dataset is in the ARFF format. We will use the ARFFFiles package to load the data into a DataFrame and show its first 7 rows.\n\ndf = ARFFFiles.load(DataFrame, file_path)\nfirst(df, 7) # show the first 7 lines\n\n7×12 DataFrame\n\n\n\nRow\nIDpol\nClaimNb\nExposure\nArea\nVehPower\nVehAge\nDrivAge\nBonusMalus\nVehBrand\nVehGas\nDensity\nRegion\n\n\n\nFloat64\nFloat64\nFloat64\nCat…\nFloat64\nFloat64\nFloat64\nFloat64\nCat…\nString\nFloat64\nCat…\n\n\n\n\n1\n1.0\n1.0\n0.1\nD\n5.0\n0.0\n55.0\n50.0\nB12\nRegular\n1217.0\nR82\n\n\n2\n3.0\n1.0\n0.77\nD\n5.0\n0.0\n55.0\n50.0\nB12\nRegular\n1217.0\nR82\n\n\n3\n5.0\n1.0\n0.75\nB\n6.0\n2.0\n52.0\n50.0\nB12\nDiesel\n54.0\nR22\n\n\n4\n10.0\n1.0\n0.09\nB\n7.0\n0.0\n46.0\n50.0\nB12\nDiesel\n76.0\nR72\n\n\n5\n11.0\n1.0\n0.84\nB\n7.0\n0.0\n46.0\n50.0\nB12\nDiesel\n76.0\nR72\n\n\n6\n13.0\n1.0\n0.52\nE\n6.0\n2.0\n38.0\n50.0\nB12\nRegular\n3003.0\nR31\n\n\n7\n15.0\n1.0\n0.45\nE\n6.0\n2.0\n38.0\n50.0\nB12\nRegular\n3003.0\nR31\n\n\n\n\n\n\nThe content of some of the columns, such as IDpol (the policy id) or ClaimNb (the number of claims), is incorrectly recognized as floating point numbers, but are actually integer valued. We will convert them to integers. This is not strictly necessary but visually much more appealing.\n\ndf[!,:IDpol] = convert.(Int, df[!,:IDpol])\ndf[!,:ClaimNb] = convert.(Int, df[!,:ClaimNb])\nfirst(df, 7)\n\n7×12 DataFrame\n\n\n\nRow\nIDpol\nClaimNb\nExposure\nArea\nVehPower\nVehAge\nDrivAge\nBonusMalus\nVehBrand\nVehGas\nDensity\nRegion\n\n\n\nInt64\nInt64\nFloat64\nCat…\nFloat64\nFloat64\nFloat64\nFloat64\nCat…\nString\nFloat64\nCat…\n\n\n\n\n1\n1\n1\n0.1\nD\n5.0\n0.0\n55.0\n50.0\nB12\nRegular\n1217.0\nR82\n\n\n2\n3\n1\n0.77\nD\n5.0\n0.0\n55.0\n50.0\nB12\nRegular\n1217.0\nR82\n\n\n3\n5\n1\n0.75\nB\n6.0\n2.0\n52.0\n50.0\nB12\nDiesel\n54.0\nR22\n\n\n4\n10\n1\n0.09\nB\n7.0\n0.0\n46.0\n50.0\nB12\nDiesel\n76.0\nR72\n\n\n5\n11\n1\n0.84\nB\n7.0\n0.0\n46.0\n50.0\nB12\nDiesel\n76.0\nR72\n\n\n6\n13\n1\n0.52\nE\n6.0\n2.0\n38.0\n50.0\nB12\nRegular\n3003.0\nR31\n\n\n7\n15\n1\n0.45\nE\n6.0\n2.0\n38.0\n50.0\nB12\nRegular\n3003.0\nR31\n\n\n\n\n\n\nUsers new to Julia, please note the dot syntax above. The dot syntax indicates that the convert function is applied elementwise to the selected columns of the dataframe df."
  },
  {
    "objectID": "posts/insurance-claim-frequency/freMTPL2freq.html#description-of-columns",
    "href": "posts/insurance-claim-frequency/freMTPL2freq.html#description-of-columns",
    "title": "Modelling the frequency of insurance claims",
    "section": "1.2 Description of columns",
    "text": "1.2 Description of columns\nThe freMTPL2freq dataset contains tabular data arranged in the following columns:\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nIDpol\nThe policy ID, which uniquely identifies policy holders.\n\n\nClaimNb\nNumber of claims during the exposure period.\n\n\nExposure\nThe time the insurance policy was held in the specific year.\n\n\nArea\nThe area code.\n\n\nVehPower\nThe power of the car (ordered categorical).\n\n\nVehAge\nThe vehicle age, in years.\n\n\nDrivAge\nThe driver age, in years (in France, people can drive a car from age 18).\n\n\nBonusMalus\nBonus/malus, between 50 and 350: &lt;100 means bonus, &gt;100 means malus.\n\n\nVehBrand\nThe car brand (unknown categories).\n\n\nVehGas\nThe car gas. Either Diesel or regular.\n\n\nDensity\nThe density of inhabitants (number of inhabitants per km²) in the city the driver lives in.\n\n\nRegion\nThe policy regions in France (based on a standard French classification)."
  },
  {
    "objectID": "posts/insurance-claim-frequency/freMTPL2freq.html#histograms-of-policy-holder-features",
    "href": "posts/insurance-claim-frequency/freMTPL2freq.html#histograms-of-policy-holder-features",
    "title": "Modelling the frequency of insurance claims",
    "section": "2.1 Histograms of policy holder features",
    "text": "2.1 Histograms of policy holder features\nA common first step in understanding a dataset is to examine histograms. These visualizations can reveal irregularities, such as outliers or unreasonable values, as well as potentially useful data transformations.\nIn this and the upcoming sections, the code used to generate plots will be hidden by default. If you wish to view the code, simply click the Code button to expand it.\n\n\nCode\nlet \n    gr()\n    regions = map(s-&gt; string(s)[2:3], df.Region)\n    plot_size = (500,300)\n    dpi = 300\n    \n    # Create a list of IOBuffer instances for each plot\n    io_buffers = [IOBuffer() for _ in 1:10]\n\n    plot(bar(sort(proportionmap(df.Area))); xlabel=\"Area code\", label=\"\", legend=:none, size=plot_size, dpi=dpi)\n    png(io_buffers[1])\n\n    plot(histogram(df.VehPower; bins=15, normalize=:pdf, xlabel=\"VehPower\", label=\"\", size=plot_size, dpi=dpi))\n    png(io_buffers[2])\n\n    plot(histogram(df.VehAge; bins=20, normalize=:pdf, xlabel=\"VehAge\", label=\"\", size=plot_size, dpi=dpi))\n    png(io_buffers[3])\n\n    plot(histogram(df.DrivAge; normalize=:pdf, bins=20, xlabel=\"DrivAge\", label=\"\", size=plot_size, dpi=dpi))\n    png(io_buffers[4])\n\n    plot(histogram(df.BonusMalus; normalize=:pdf, xlabel=\"BonusMalus\", label=\"\", size=plot_size, dpi=dpi))\n    png(io_buffers[5])\n\n    plot(bar(proportionmap(df.VehBrand); xlabel=\"VehBrand\", label=\"\", size=plot_size, dpi=dpi))\n    png(io_buffers[6])\n\n    plot(bar(proportionmap(df.VehGas); xlabel=\"VehGas\", label=\"\", size=plot_size, dpi=dpi))\n    png(io_buffers[7])\n\n    plot(histogram(df.Exposure; bins=10, normalize=:pdf, xlabel=\"Exposure\", label=\"\", size=plot_size, dpi=dpi))\n    png(io_buffers[8])\n\n    plot(histogram(df.Density; bins=100, normalize=:pdf, xlabel=\"Density\", label=\"\", size=plot_size, dpi=dpi))\n    png(io_buffers[9])\n\n    plot(bar(sort(proportionmap(regions)); label=\"\", xlabel=\"Region\", size=plot_size, dpi=dpi))\n    png(io_buffers[10])\n\n    plot((plot(load(io)) for io in io_buffers)...;\n        axis=nothing,\n        border=:none,\n        layout=(1,10),\n        size=(500*10,300),\n        label=\"\",\n    )\nend\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\nUse the horizontal slider to navigate through the histograms of various policyholder properties.\nBased on these histograms, we can make the following observations:\n\nVehAge exhibits a long tail, with some vehicle ages extending up to 100.\n\nBonusMalus is sharply peaked at 50 and at certain integer values up to 100, with only a few values exceeding 100.\n\nThere are Exposure values that exceed 1.\n\nDensity is skewed towards and peaked at zero, with a potential exponentially decaying tail.\n\nThese observations suggest the following:\n\nExtremely high vehicle ages may be erroneous.\n\nExposure values greater than 1 are likely mislabeled, as the dataset exclusively contains one year’s insurance data.\n\nDensity may be more appropriately modeled as a logarithmic variable.\n\n\n\nCode\nhistogram(log10.(df.Density); \n    bins=50, \n    xticks=(collect(1:4), [10, 100, 1000, 10000]),\n    label=\"\",\n    xlabel=L\"$\\log_{10}($Density$)$\"\n)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndeed, the density seems more tightly concentrated on a logarithmic scale."
  },
  {
    "objectID": "posts/insurance-claim-frequency/freMTPL2freq.html#correlations-of-the-features",
    "href": "posts/insurance-claim-frequency/freMTPL2freq.html#correlations-of-the-features",
    "title": "Modelling the frequency of insurance claims",
    "section": "2.2 Correlations of the features",
    "text": "2.2 Correlations of the features\nIn the following analysis, we will investigate the correlations between numerical features in the dataset to better understand their relationships. An important tool for this task is the Spearman correlation coefficient.\n\nSpearman correlation coefficient\nThe Spearman correlation coefficient (\\(\\rho\\)) measures the strength and direction of the relationship between two variables based on their ranks, rather than their actual values. It shows how well one variable can be described as increasing or decreasing consistently with the other, even if the relationship is not linear.\nInterpretation\n\n\\(\\rho = 1\\) : Perfect positive correlation.\n\\(\\rho = -1\\) : Perfect negative correlation.\n\\(\\rho = 0\\) : No correlation.\n\n\n\n\n\n\n\nThe Spearman correlation coefficient in mathematical formulas\n\n\n\n\n\nThe Spearman correlation coefficient is the correlation between the ranks of two data series \\((X_i)_i\\) and \\((Y_i)_i\\). Equivalently, it is the Pearson correlation coefficient between the rank variables of \\((X_i)_i\\) and \\((Y_i)_i\\). In formula, it can be expressed as\n\\[ \\rho = \\frac{\\operatorname{cov}(R[X], R[Y])}{\\sigma_{R[X]} \\sigma_{R[Y]}} \\]\nwhere:\n\nthe rank function \\(R\\) assigns to each value \\(X_i\\) its position (rank) in the ordered collection of \\(X_i\\)’s. E.g., for \\(X_1=0.3\\), \\(X_2=1.2\\) and \\(X_3=-0.1\\) the ranks \\(R\\) are \\(R[X_1]=2\\), \\(R[X_2]=3\\) and \\(R[X_3]=1\\). They are the positions of the values \\(X_i\\) in the sorted list of \\(X_i\\)’s.\n\\(\\operatorname{cov}(R[X], R[Y])\\) denotes the covariance between the sequences \\(R[X_i]\\) and \\(R[Y_i]\\) \\[ \\operatorname{cov}(R[X], R[Y]) = \\frac{1}{n-1} \\sum_i (R[X_i] - \\mu_{R[X]})(R[Y_i] - \\mu_{R[Y]})\\] with \\(\\mu_{R[X]}\\) and \\(\\mu_{R[Y]}\\) being the mean of the ranks and \\(n\\) the number of observations.\n\\(\\sigma_{R[X]}\\) and \\(\\sigma_{R[Y]}\\) are the standard deviations of the ranks of \\(X\\) and \\(Y\\), respectively. They are calculated as \\[ \\sigma_{R[X]} = \\sqrt{\\frac{1}{n-1} \\sum_i (R[X_i] - \\mu_{R[X]})^2}.\\]\n\n\n\n\nUsing the Spearman coefficient excludes the possibility of analyzing correlations involving categorical variables. Therefore, we limit the analysis to numerical features, with the exception of the area code. We hypothesize that the area codes might be associated with the population density in those regions; however, this is currently just an assumption. To test this hypothesis, we will assign numerical values to the area codes A-F, enumerating them as 1 through 6.\n\narea_as_int = [\n    findfirst(l-&gt;l==areacode, [\"A\", \"B\",\"C\",\"D\",\"E\",\"F\"]) \n    for areacode in df.Area\n];\n\nThe features VehGas, VehicleBrand, and Region are categorical and will be excluded from the correlation matrix.\nLet us calculate the Spearman correlation coefficients of all pairs of numerical features utilizing the corspearman function from the StatsBase package. We collect the pairwise coefficients in a matrix. This resulting matrix is symmetric as the correlations between \\(X\\) and \\(Y\\) are the same as between \\(Y\\) and \\(X\\). This follows directly from the definition of the correlation coefficient.\n\nfeature_corspearman = let\n    features = [:VehPower, :VehAge, :DrivAge, :BonusMalus, :Density]\n    coefs = zeros(length(features)+1, length(features)+1)\n\n    for j in eachindex(features)\n        coefs[1,j+1] = corspearman(df[!,features[j]], area_as_int)\n    end\n    \n    for i in 1:length(features)\n        for j in 1:length(features)\n            i &gt;= j && continue\n            coefs[i+1,j+1] = corspearman(df[!,features[i]], df[!,features[j]])\n        end\n    end\n    coefs = round.(coefs, digits=2)\n    coefs += coefs'\n    for i in 1:size(coefs,1)\n        coefs[i,i] = 1\n    end\n    coefs\nend\n\n6×6 Matrix{Float64}:\n  1.0   -0.01  -0.1   -0.05   0.14   0.98\n -0.01   1.0    0.0    0.04  -0.07  -0.01\n -0.1    0.0    1.0   -0.08   0.08  -0.1\n -0.05   0.04  -0.08   1.0   -0.57  -0.04\n  0.14  -0.07   0.08  -0.57   1.0    0.14\n  0.98  -0.01  -0.1   -0.04   0.14   1.0\n\n\nWe visualize this matrix as a heatmap. The heatmap is symmetric along the diagonal, with the diagonal elements being equal to 1. The color of each cell represents the strength and direction of the correlation between the corresponding pair of features.\n\n\nCode\nlet\n    tick_labels = [\"Area\", \"VehPower\", \"VehAge\", \"DrivAge\", \"BonusMalus\", \"Density\"]\n    n = length(tick_labels)\n    heatmap(feature_corspearman;\n        color=:vik,\n        clims=(-1,1),\n        ticks=(eachindex(tick_labels), tick_labels),\n        title=\"Spearman correlation\",\n        xrot=90,\n        yflip=true,\n    )\n    for i in 1:n, j in 1:n\n        color = abs(feature_corspearman[i,j]) &gt; 0.8 ? :white : :black\n        annotate!(i,j, text(round(feature_corspearman[i,j],digits=3), 8,\"Computer Modern\",color))\n    end\n    plot!(\n        aspectratio=1,\n        lims=(0.5, n+0.5),\n        bottommargin=5Plots.PlotMeasures.mm,\n    )\nend\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe spearman table reveals a positive correlation between area and density and a negative corrleation between driver age and BonusMalus. The correlation between the other features is weak. Next, we will visualize the correlation between the area and the density as well as the driver age and the BonusMalus.\n\n\nCorrelations visualized\n\n\nCode\nlet df = df\n    df = select(df, :Area, :Density)\n    df = groupby(df, :Area)\n    df = combine(df,\n        :Density=&gt;mean,\n        :Density=&gt;std,\n        :Density=&gt;(c-&gt;exp(mean(log, c)))=&gt;:Density_log_mean, \n    )\n    sort!(df)\n    \n    plot(df.Area, df.Density_mean;\n        marker=:circle,\n        label=\"average density per area code\",\n    )\n    plot!(df.Area, df.Density_mean .+ df.Density_std; label=\"1 standard deviation\", color=1, ls=:dash)\n    plot!(df.Area, df.Density_mean .- df.Density_std; label=\"\", color=1, ls=:dash)\n\n    plot!(df.Area, df.Density_log_mean; label=\"averaged log density per area code\", color=2)\n    plot!(\n        yscale=:log10,\n        xlabel=\"Area\",\n        ylabel=\"density (log)\",\n    )\n    \nend\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe observe that the \\(log\\)(density) is linearly increasing with the area code. Hence, our initial guess was correct and the area code is mainly determined by the density of people living in that area.\n\n\nCode\nlet df = df\n\n    avrg_bonus_malus = mean(df.BonusMalus)\n    df = select(df, :DrivAge, :BonusMalus)\n    df = groupby(df, :DrivAge)\n    df = combine(df,\n        :BonusMalus=&gt;mean,\n        :BonusMalus=&gt;std,\n    )\n    plot(df.DrivAge, df.BonusMalus_mean;\n        marker=:circle,\n        label=\"average BonusMalus per driver age\",\n    )\n    plot!(df.DrivAge, df.BonusMalus_mean .+ df.BonusMalus_std; label=\"1 standard deviatation\", color=1, ls=:dash)\n    plot!(df.DrivAge, df.BonusMalus_mean .- df.BonusMalus_std; label=\"\", color=1, ls=:dash)\n    hline!([avrg_bonus_malus]; label=\"average BonusMalus\")\n    plot!(\n        xlabel=\"driver age\",\n        ylabel=\"bonus malus\",\n    )\nend\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe driver age is negatively correlated with the BonusMalus. This is also expected, as the BonusMalus is a measure of the driver’s driving history. The older the driver, the longer they typically hold the insurance and, providing no claims have been made, the lower the BonusMalus."
  },
  {
    "objectID": "posts/insurance-claim-frequency/freMTPL2freq.html#distribution-of-the-predicted-variable",
    "href": "posts/insurance-claim-frequency/freMTPL2freq.html#distribution-of-the-predicted-variable",
    "title": "Modelling the frequency of insurance claims",
    "section": "2.3 Distribution of the predicted variable",
    "text": "2.3 Distribution of the predicted variable\nOur goal in this notebook is to predict the number of claims ClaimNb per year as a function of to be chosen features. Claims from different policy holders are (to a very good approximation) independent of each other. Hence, the distribuion of ClaimNb is typically considered Poissonian. We will check this by plotting a histogram of the ClaimNb values.\nThe Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time. It is defined by the rate parameter \\(\\lambda\\) which is the average number of events in the interval. In our case, it is the average number of claims per year.\n\npoisson = Poisson(mean(df.ClaimNb))\n\nPoisson{Float64}(λ=0.05324676665491664)\n\n\n\n\nCode\nhistogram(df.ClaimNb; \n    normalize=:probability,\n    label=\"ClaimNb\",\n    xlabel=\"Number of claims\",\n    binwidth=20,\n)\nplot!(0:4, pdf.(poisson, (0:4));\n    label=\"Poisson distribution\",\n)\n\nplot!(proportionmap(df.ClaimNb);\n    inset=(1, bbox(0.35, 0.2, 0.6, 0.6)),\n    subplot=2,\n    marker=:circle,\n    label=\"ClaimNb\",\n)\nplot!(0:4, pdf.(poisson, (0:4)); \n    subplot=2,\n    label=\"Poisson distribution\",\n    yscale=:log10\n    )\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe number of claims per year ClaimNb is very well approximated by a Poisson distribution for small number of claims. For larger numbers of claims, the Poisson distribution is not a good approximation. We actually observe that the tail is not exponential but flattens out. Because the number of claims are only collected over an exposure period of maximmally one year, large number of claims should be treatened with caution. In fact, the ratio between policy holders with more than 4 vs. policy holders with any (non-zero) number of claims is less than 0.03%:\n\nsum(df.ClaimNb .&gt; 4) / sum(df.ClaimNb .&gt; 0) * 100\n\n0.026423957721667647\n\n\nThese excessive numbers of claims might be due to mislabeled data or fraud."
  },
  {
    "objectID": "posts/insurance-claim-frequency/freMTPL2freq.html#frequency-as-a-function-of-the-features",
    "href": "posts/insurance-claim-frequency/freMTPL2freq.html#frequency-as-a-function-of-the-features",
    "title": "Modelling the frequency of insurance claims",
    "section": "2.4 Frequency as a function of the features",
    "text": "2.4 Frequency as a function of the features\nWe aim to model the frequency of claims as a function of the features. To build an intuition about the relationship between the features and the frequency, we will visualize the frequency as a function of each feature.\nTo begin with, the portfolio frequency is:\n\nportfolio_frequency = sum(df.ClaimNb) / sum(df.Exposure)\n\n0.10070308464041308\n\n\nNext, we will plot the frequency as a function of the \\(log\\)(density), Area and BonusMalus.\n\n\nCode\nlet df = df\n\n\n    \"\"\"\n    confint_poisson(n, t)\n\n    n :: number of observations\n\n    t :: total time at risk\n    \"\"\"\n    function confint_poisson(n::Integer, t::Real)\n\n        if n == 0\n            return zero(t), zero(t)\n        else\n            l = quantile(Chisq(2*n), 0.025)/2 / t\n            u = quantile(Chisq(2*n+2), 0.975)/2 / t\n        end\n\n        l, u\n    end\n\n    df = select(df, :Density, :ClaimNb, :Exposure)\n\n    # Bin the density data into 15 bins on a log scale\n    fmt(from, to, i; leftclosed, rightclosed) = (from + to)/2\n    df.density_bin = cut(df.Density, 10 .^ range(0,5,length=15), labels=fmt)\n\n    # Calculate the frequency for each bin\n    # as the total claims divided by the total exposure\n    df = groupby(df, :density_bin)\n    df = combine(df, :ClaimNb=&gt;sum, :Exposure=&gt;sum)\n    df.Frequency_bin = df.ClaimNb_sum ./ df.Exposure_sum\n\n    xvalues = unwrap.(df.density_bin)\n    plot(xvalues, df.Frequency_bin;\n        marker=:circle,\n        label=\"Frequency per density bin\",\n    )\n\n    # Calculate the confidence intervals \n    confints = confint_poisson.(df.ClaimNb_sum, df.Exposure_sum)\n    plot!(xvalues, [c[1] for c in confints];\n        color=1,\n        ls=:dash,\n        label=\"95% - confidence interval\",\n    )\n    plot!(xvalues, [c[2] for c in confints];\n        color=1,\n        ls=:dash,\n        label=\"\",\n    )\n\n    hline!([portfolio_frequency],\n        color=2,\n        label=\"portfolio frequency\"\n    )\n\n    plot!(\n        xscale=:log10, \n        xlims=(1, 26000), \n        xlabel=\"Density (log)\",\n        ylabel=\"Frequency\",\n    )\n    \nend\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe frequency of claims appears to increase linearly with the \\(log\\)(density) for values of density greater than 10. However, caution is required when interpreting the very small density values, as indicated by the large error bars represented by the dashed line.\n\n\nCode\nlet df = df\n    df = select(df, :Area, :ClaimNb, :Exposure)\n    df = groupby(df, :Area)\n    df = combine(df,:ClaimNb=&gt;sum, :Exposure=&gt;sum)\n    df.Frequency_mean_bin = df.ClaimNb_sum ./ df.Exposure_sum \n    sort!(df)\n    plot(df.Area, df.Frequency_mean_bin;\n        marker=:circle,\n        label=\"\",\n    )\n\n    hline!([portfolio_frequency],\n        color=2,\n        label=\"portfolio frequency\"\n    )\n\n    \n    plot!(\n        ylims=(0,0.2),\n        xlabel=\"Area\",\n        ylabel=\"Frequency\",\n    )\n    \nend\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe frequency appears to increase with the area code (interpreted as ordinal variable with values 1 to 6) as well.\n\n\nCode\nlet df = df\n\n    bms = [100]\n    for i in 1:13\n        push!(bms, max.(floor(Int, 0.95*bms[i]), 50))\n    end\n    no_claim_bms = reverse(bms[2:end])\n    \n    df = select(df, :BonusMalus, :ClaimNb, :Exposure)\n    df = groupby(df, :BonusMalus)\n    df = combine(df,:ClaimNb=&gt;sum, :Exposure=&gt;sum)\n    df.Frequency_mean_bin = df.ClaimNb_sum ./ df.Exposure_sum \n    sort!(df)\n    scatter(df.BonusMalus, df.Frequency_mean_bin;\n        marker=:circle,\n        label=\"\",\n    )\n\n    plot!(no_claim_bms, [df.Frequency_mean_bin[findfirst(x-&gt;x==bm, df.BonusMalus)] for bm in no_claim_bms];\n        label=\"likely no claim track\",\n        color=3,\n        ls=:dash,\n    )\n    \n\n    hline!([portfolio_frequency]; label=\"portfolio frequency\", color=2)\n    plot!(\n        ylims=(0,0.5),\n        xlims=(49, 101),\n        xlabel=\"BonusMalus\",\n        ylabel=\"Frequency\",\n    )\n    \nend\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe observe a slight increase of the frequency with BonusMalus. But the datapoints are very scattered. The reason why most of the datapoints are above the portfolio frequency is that most policy holders have a BonusMalus of 50. Hence, the leftmost datapoint has very high weight and is below the portfolio frequency.\nThe green dashed line indicates drivers, which have not made a claim during their exposure period. In a follow up blog post I will show how to find out these policy holders and that these drivers have smaller frequency than the remainder. The key observation in the follow up blog post is that BonusMalus - in contrast to the other features - does contain information about the frequency of claims of the past years. Stay tuned!"
  },
  {
    "objectID": "posts/insurance-claim-frequency/freMTPL2freq.html#prune-the-dataset",
    "href": "posts/insurance-claim-frequency/freMTPL2freq.html#prune-the-dataset",
    "title": "Modelling the frequency of insurance claims",
    "section": "3.1 Prune the dataset",
    "text": "3.1 Prune the dataset\nBased on our analysis in the previous section, we will preprocess the dataset in the following way:\n\n\n\n\n\n\n\nPreprocessing Step\nJustification\n\n\n\n\nMap the area code to integers\nWe observed a strong correlation between the area codes mapped to integers between 1-6 and the corresponding population density.\n\n\nCap the vehicle age at 25\nThere are only very few vehicle ages above 25. These are either mislabeled data or historic cars, which are typically used differently than regular cars.\n\n\nCap the exposure at 1\nThe whole dataset only contains insurance data of one year. The exposure values greater than 1 are mislabeled data.\n\n\nCap the claim numbers at 4\nLess than 0.03 % of policy holders have more than 4 claims per year. Some of these might be mislabeled data.\n\n\n\n\ndf.Area = area_as_int\ndf.VehAge = min.(25, df.VehAge)\ndf.Exposure = min.(1, df.Exposure)\ndf.ClaimNb = min.(4, df.ClaimNb)\ndf\n\n678013×12 DataFrame677988 rows omitted\n\n\n\nRow\nIDpol\nClaimNb\nExposure\nArea\nVehPower\nVehAge\nDrivAge\nBonusMalus\nVehBrand\nVehGas\nDensity\nRegion\n\n\n\nInt64\nInt64\nFloat64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nCat…\nString\nFloat64\nCat…\n\n\n\n\n1\n1\n1\n0.1\n4\n5.0\n0.0\n55.0\n50.0\nB12\nRegular\n1217.0\nR82\n\n\n2\n3\n1\n0.77\n4\n5.0\n0.0\n55.0\n50.0\nB12\nRegular\n1217.0\nR82\n\n\n3\n5\n1\n0.75\n2\n6.0\n2.0\n52.0\n50.0\nB12\nDiesel\n54.0\nR22\n\n\n4\n10\n1\n0.09\n2\n7.0\n0.0\n46.0\n50.0\nB12\nDiesel\n76.0\nR72\n\n\n5\n11\n1\n0.84\n2\n7.0\n0.0\n46.0\n50.0\nB12\nDiesel\n76.0\nR72\n\n\n6\n13\n1\n0.52\n5\n6.0\n2.0\n38.0\n50.0\nB12\nRegular\n3003.0\nR31\n\n\n7\n15\n1\n0.45\n5\n6.0\n2.0\n38.0\n50.0\nB12\nRegular\n3003.0\nR31\n\n\n8\n17\n1\n0.27\n3\n7.0\n0.0\n33.0\n68.0\nB12\nDiesel\n137.0\nR91\n\n\n9\n18\n1\n0.71\n3\n7.0\n0.0\n33.0\n68.0\nB12\nDiesel\n137.0\nR91\n\n\n10\n21\n1\n0.15\n2\n7.0\n0.0\n41.0\n50.0\nB12\nDiesel\n60.0\nR52\n\n\n11\n25\n1\n0.75\n2\n7.0\n0.0\n41.0\n50.0\nB12\nDiesel\n60.0\nR52\n\n\n12\n27\n1\n0.87\n3\n7.0\n0.0\n56.0\n50.0\nB12\nDiesel\n173.0\nR93\n\n\n13\n30\n1\n0.81\n4\n4.0\n1.0\n27.0\n90.0\nB12\nRegular\n695.0\nR72\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n678002\n6114319\n0\n0.00547945\n3\n4.0\n0.0\n61.0\n50.0\nB12\nRegular\n205.0\nR25\n\n\n678003\n6114320\n0\n0.00273973\n5\n10.0\n0.0\n29.0\n80.0\nB12\nDiesel\n2471.0\nR11\n\n\n678004\n6114321\n0\n0.00547945\n5\n4.0\n0.0\n29.0\n80.0\nB12\nRegular\n5360.0\nR11\n\n\n678005\n6114322\n0\n0.00547945\n5\n11.0\n0.0\n49.0\n74.0\nB12\nDiesel\n5360.0\nR11\n\n\n678006\n6114323\n0\n0.00547945\n4\n4.0\n0.0\n34.0\n80.0\nB12\nRegular\n731.0\nR82\n\n\n678007\n6114324\n0\n0.00547945\n4\n11.0\n0.0\n41.0\n50.0\nB12\nDiesel\n528.0\nR93\n\n\n678008\n6114325\n0\n0.00547945\n5\n6.0\n4.0\n40.0\n68.0\nB12\nRegular\n2733.0\nR93\n\n\n678009\n6114326\n0\n0.00273973\n5\n4.0\n0.0\n54.0\n50.0\nB12\nRegular\n3317.0\nR93\n\n\n678010\n6114327\n0\n0.00273973\n5\n4.0\n0.0\n41.0\n95.0\nB12\nRegular\n9850.0\nR11\n\n\n678011\n6114328\n0\n0.00273973\n4\n6.0\n2.0\n45.0\n50.0\nB12\nDiesel\n1323.0\nR82\n\n\n678012\n6114329\n0\n0.00273973\n2\n4.0\n0.0\n60.0\n50.0\nB12\nRegular\n95.0\nR26\n\n\n678013\n6114330\n0\n0.00273973\n2\n7.0\n6.0\n29.0\n54.0\nB12\nDiesel\n65.0\nR72"
  },
  {
    "objectID": "posts/insurance-claim-frequency/freMTPL2freq.html#split-into-training-and-test-set",
    "href": "posts/insurance-claim-frequency/freMTPL2freq.html#split-into-training-and-test-set",
    "title": "Modelling the frequency of insurance claims",
    "section": "3.2 Split into training and test set",
    "text": "3.2 Split into training and test set\nIt is very important to not split the dataset into training and test set before doing any data preprocessing. Otherwise, we might introduce a bias in the model. We will split the dataset into a training and a test set with a ratio of 80:20.\n\ndf_train, df_test = let\n    \n    idxs = shuffle!(collect(axes(df, 1)))\n    idx_split = ceil(Int, length(idxs) * 0.8)\n    df[idxs[1:idx_split],:], df[idxs[idx_split+1:end],:]\nend\n\n\n(542411×12 DataFrame\n    Row │ IDpol    ClaimNb  Exposure  Area   VehPower  VehAge   DrivAge  Bonus ⋯\n        │ Int64    Int64    Float64   Int64  Float64   Float64  Float64  Float ⋯\n────────┼───────────────────────────────────────────────────────────────────────\n      1 │ 3151433        0      0.51      1       7.0      3.0     49.0        ⋯\n      2 │ 3213399        1      0.64      3      10.0      7.0     46.0\n      3 │ 1044219        0      1.0       6       4.0     15.0     64.0\n      4 │ 4035567        0      0.61      5       5.0      1.0     51.0\n      5 │ 4044984        0      0.9       3       4.0      0.0     43.0        ⋯\n      6 │ 1195413        0      0.49      5       9.0      3.0     35.0\n      7 │ 2122535        0      1.0       3       8.0     10.0     39.0\n      8 │ 2110683        0      0.64      6      15.0      1.0     28.0\n   ⋮    │    ⋮        ⋮        ⋮        ⋮       ⋮         ⋮        ⋮         ⋮ ⋱\n 542405 │ 4046999        0      0.9       1       8.0      2.0     34.0        ⋯\n 542406 │ 6064316        0      0.74      5       5.0      2.0     69.0\n 542407 │ 4002534        0      0.02      5       4.0     10.0     73.0\n 542408 │ 1127531        0      0.85      4       7.0      1.0     53.0\n 542409 │ 1066899        0      0.16      5       7.0      9.0     47.0        ⋯\n 542410 │ 4021344        0      0.12      3       4.0      2.0     60.0\n 542411 │ 1113673        0      0.38      3       7.0     12.0     33.0\n                                               5 columns and 542396 rows omitted, 135602×12 DataFrame\n    Row │ IDpol    ClaimNb  Exposure  Area   VehPower  VehAge   DrivAge  Bonus ⋯\n        │ Int64    Int64    Float64   Int64  Float64   Float64  Float64  Float ⋯\n────────┼───────────────────────────────────────────────────────────────────────\n      1 │ 1059728        0      0.06      4       7.0      4.0     27.0        ⋯\n      2 │ 6095889        0      0.37      6      10.0      1.0     50.0\n      3 │ 2283168        0      0.08      5       4.0      5.0     45.0\n      4 │   68162        1      1.0       3       8.0      9.0     32.0\n      5 │ 4046317        0      0.15      3       5.0      0.0     33.0        ⋯\n      6 │ 3036715        0      0.44      3      10.0      1.0     28.0\n      7 │ 4186963        0      0.38      3       6.0      8.0     55.0\n      8 │ 1123757        0      0.16      5       5.0     12.0     28.0\n   ⋮    │    ⋮        ⋮        ⋮        ⋮       ⋮         ⋮        ⋮         ⋮ ⋱\n 135596 │ 4011064        0      0.45      5       7.0      0.0     31.0        ⋯\n 135597 │ 4141749        0      1.0       4       7.0     16.0     25.0\n 135598 │  144576        0      1.0       4       5.0      1.0     64.0\n 135599 │ 3121513        0      1.0       3       7.0     10.0     50.0\n 135600 │ 1049184        0      1.0       4       5.0      9.0     52.0        ⋯\n 135601 │ 3115159        0      0.76      1       5.0      6.0     61.0\n 135602 │ 4033126        0      0.47      3      12.0      9.0     36.0\n                                               5 columns and 135587 rows omitted)"
  },
  {
    "objectID": "posts/insurance-claim-frequency/freMTPL2freq.html#base-model",
    "href": "posts/insurance-claim-frequency/freMTPL2freq.html#base-model",
    "title": "Modelling the frequency of insurance claims",
    "section": "4.1 Base model",
    "text": "4.1 Base model\nBefore engineering a sophisticated model, we will start with a very simple baseline model. We will use the ClaimNb ~ 1 model, which predicts the average number of claims per year for all policy holders. Predictions of this model equal the portfolio frequency.\n\nglm_base = glm(\n    @formula(ClaimNb ~ 1), df_train, Poisson(), LogLink(); \n    offset=df_train.Exposure\n)\n\nStatsModels.TableRegressionModel{GeneralizedLinearModel{GLM.GlmResp{Vector{Float64}, Poisson{Float64}, LogLink}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\nClaimNb ~ 1\n\nCoefficients:\n──────────────────────────────────────────────────────────────────────────\n                Coef.  Std. Error        z  Pr(&gt;|z|)  Lower 95%  Upper 95%\n──────────────────────────────────────────────────────────────────────────\n(Intercept)  -3.52948  0.00588705  -599.53    &lt;1e-99   -3.54102   -3.51794\n──────────────────────────────────────────────────────────────────────────\n\n\nThe minimized loss function is the Poisson loss function: \\[\n\\frac{2}{N} \\sum_{i=1}^N \\left[ y_i - \\hat{y}_i + y_i (\\log (y_i) - \\log (\\hat{y}_i))\\right],\n\\] where \\(y_i\\) is the true number of claims per year and \\(\\hat{y}_i\\) is the predicted number of claims per year. We average over the sample size \\(N\\) to be able to compare between different dataset sizes, e.g. train and test set.\n\nfunction poisson_loss(pred, target)\n    s = 0\n    for i in eachindex(pred, target)\n        p = pred[i]\n        t = target[i]\n        s += p - t\n        iszero(t) && continue # zero t is not contrinuting to the sum\n        s += t*log(t / p)\n    end\n    2*s / length(pred) # divide by the sample size\nend\n\npoisson_loss (generic function with 1 method)\n\n\nNext, lets investigate the loss of the base model on the training and test set. Besides the Poisson loss, we will also calculate the mean absolute error (MAE) and the mean squared error (MSE) for comparison. The function definition is hidden in the next cell. To inspect the code, click on the “Code” button below.\n\n\nCode\ncalculate_losses() = calculate_losses([], [])\nfunction calculate_losses(models, model_names)\n\n    datasets = [df_train, df_test]\n\n    df = DataFrame(\"Models\"=&gt;vcat([\"Base\"], model_names...))\n    for i in 1:2\n        dataset = datasets[i]\n        name = i==1 ? \"Train\" : \"Test\"\n\n        losses = [poisson_loss(exp(coef(glm_base)[]) .* exp.(dataset.Exposure), dataset.ClaimNb)]\n        append!(losses, [poisson_loss(predict(m, dataset; offset=dataset.Exposure), dataset.ClaimNb) for m in models])\n        df[!, name*\"_\"*\"PoissonLoss\"] = losses\n        for loss in [L1DistLoss(), L2DistLoss()]\n            losses = [mean(loss, exp(coef(glm_base)[]) .* exp.(dataset.Exposure), dataset.ClaimNb)]\n            append!(losses, [mean(loss, predict(m, dataset; offset=dataset.Exposure), dataset.ClaimNb) for m in models])\n            loss_name = string(loss)\n            df[!, name*\"_\"*loss_name] = losses\n        end\n    end\n    df\nend\n\n\ncalculate_losses (generic function with 2 methods)\n\n\n\ncalculate_losses()\n\n1×7 DataFrame\n\n\n\nRow\nModels\nTrain_PoissonLoss\nTrain_L1DistLoss\nTrain_L2DistLoss\nTest_PoissonLoss\nTest_L1DistLoss\nTest_L2DistLoss\n\n\n\nString\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nBase\n0.31513\n0.10035\n0.0562994\n0.316145\n0.100531\n0.0566815\n\n\n\n\n\n\nThe absolute numbers are pretty meaningless. So, lets fit a linear regression model with all available features and compare it to the base model."
  },
  {
    "objectID": "posts/insurance-claim-frequency/freMTPL2freq.html#model-including-all-features",
    "href": "posts/insurance-claim-frequency/freMTPL2freq.html#model-including-all-features",
    "title": "Modelling the frequency of insurance claims",
    "section": "4.2 Model including all features",
    "text": "4.2 Model including all features\n\nglm_1 = glm(\n    @formula(\n        ClaimNb ~   Area + VehPower + VehAge + DrivAge + \n                    BonusMalus + VehBrand + VehGas + log(Density) \n                    + Region\n        ),\n    df_train, Poisson(), LogLink(); \n    offset=df_train.Exposure\n)\n\nStatsModels.TableRegressionModel{GeneralizedLinearModel{GLM.GlmResp{Vector{Float64}, Poisson{Float64}, LogLink}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\nClaimNb ~ 1 + Area + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + :(log(Density)) + Region\n\nCoefficients:\n──────────────────────────────────────────────────────────────────────────────────────\n                       Coef.   Std. Error       z  Pr(&gt;|z|)    Lower 95%     Upper 95%\n──────────────────────────────────────────────────────────────────────────────────────\n(Intercept)      -5.28691     0.0590539    -89.53    &lt;1e-99  -5.40266     -5.17117\nArea              0.0296931   0.0180125      1.65    0.0993  -0.00561062   0.0649969\nVehPower          0.00888761  0.0030716      2.89    0.0038   0.00286738   0.0149078\nVehAge           -0.0363126   0.00130325   -27.86    &lt;1e-99  -0.038867    -0.0337583\nDrivAge           0.00917683  0.000443995   20.67    &lt;1e-94   0.00830662   0.010047\nBonusMalus        0.0202785   0.000352037   57.60    &lt;1e-99   0.0195886    0.0209685\nVehBrand: B10    -0.00974951  0.0411603     -0.24    0.8128  -0.0904221    0.0709231\nVehBrand: B11     0.0320774   0.0446475      0.72    0.4725  -0.05543      0.119585\nVehBrand: B12     0.0591598   0.0196045      3.02    0.0025   0.0207357    0.097584\nVehBrand: B13     0.038082    0.0457222      0.83    0.4049  -0.0515318    0.127696\nVehBrand: B14    -0.114017    0.0863138     -1.32    0.1865  -0.283189     0.0551552\nVehBrand: B2      0.00469451  0.0171327      0.27    0.7841  -0.028885     0.038274\nVehBrand: B3     -0.00149352  0.0245149     -0.06    0.9514  -0.0495418    0.0465548\nVehBrand: B4     -0.0104469   0.0329764     -0.32    0.7514  -0.0750794    0.0541856\nVehBrand: B5      0.102717    0.0275759      3.72    0.0002   0.0486695    0.156765\nVehBrand: B6     -0.0522967   0.0318823     -1.64    0.1009  -0.114785     0.0101914\nVehGas: Regular   0.0786651   0.0122417      6.43    &lt;1e-09   0.0546719    0.102658\nlog(Density)      0.0109908   0.0134249      0.82    0.4130  -0.0153215    0.037303\nRegion: R21       0.174104    0.0882262      1.97    0.0485   0.00118437   0.347025\nRegion: R22       0.0811899   0.0571288      1.42    0.1553  -0.0307804    0.19316\nRegion: R23      -0.201735    0.0673781     -2.99    0.0028  -0.333793    -0.0696759\nRegion: R24       0.165239    0.0260858      6.33    &lt;1e-09   0.114112     0.216367\nRegion: R25       0.131155    0.0487188      2.69    0.0071   0.0356678    0.226642\nRegion: R26      -0.0130556   0.0541618     -0.24    0.8095  -0.119211     0.0930995\nRegion: R31      -0.121536    0.0378589     -3.21    0.0013  -0.195738    -0.0473345\nRegion: R41      -0.123923    0.0489883     -2.53    0.0114  -0.219938    -0.0279074\nRegion: R42       0.132116    0.0959853      1.38    0.1687  -0.0560115    0.320244\nRegion: R43      -0.132125    0.15214       -0.87    0.3852  -0.430315     0.166065\nRegion: R52       0.0731029   0.0324227      2.25    0.0242   0.00955562   0.13665\nRegion: R53       0.223018    0.030423       7.33    &lt;1e-12   0.16339      0.282646\nRegion: R54       0.00290103  0.0427018      0.07    0.9458  -0.0807929    0.086595\nRegion: R72      -0.0708156   0.0362951     -1.95    0.0510  -0.141953     0.000321556\nRegion: R73      -0.139942    0.0470609     -2.97    0.0029  -0.232179    -0.0477041\nRegion: R74       0.151976    0.0761753      2.00    0.0460   0.00267492   0.301277\nRegion: R82       0.133687    0.0251289      5.32    &lt;1e-06   0.0844351    0.182938\nRegion: R83      -0.30446     0.0847398     -3.59    0.0003  -0.470546    -0.138373\nRegion: R91      -0.0784402   0.0352373     -2.23    0.0260  -0.147504    -0.00937636\nRegion: R93       0.0159498   0.0261731      0.61    0.5423  -0.0353485    0.0672482\nRegion: R94       0.0878118   0.0750628      1.17    0.2421  -0.0593086    0.234932\n──────────────────────────────────────────────────────────────────────────────────────\n\n\n\ncalculate_losses([glm_1], [\"all_features\"])\n\n2×7 DataFrame\n\n\n\nRow\nModels\nTrain_PoissonLoss\nTrain_L1DistLoss\nTrain_L2DistLoss\nTest_PoissonLoss\nTest_L1DistLoss\nTest_L2DistLoss\n\n\n\nString\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nBase\n0.31513\n0.10035\n0.0562994\n0.316145\n0.100531\n0.0566815\n\n\n2\nall_features\n0.307316\n0.0994627\n0.0557037\n0.308429\n0.0997102\n0.0560861\n\n\n\n\n\n\nWe observe that the Poisson regression model with all features has a lower Poisson loss, MAE and MSE than the base model - on the training dataset and on the test dataset. This should not be surprising. Any decent model should perform better than just the average number of claims.\nBut, investigating the \\(p\\) and \\(z\\)-values of the regression table reveals that not all features appear significant. For example, both the \\(log\\)(density) and the Area have a high \\(p\\) and low \\(z\\) value. In the beginning of this notebook, we observed that these features have a linear relationship with number of claims and have a large Spearman correlation coefficient. Hence, having both features in the model is redundandent - we should only include one. Next, we fit a GLM with all features but the Area."
  },
  {
    "objectID": "posts/insurance-claim-frequency/freMTPL2freq.html#model-without-area",
    "href": "posts/insurance-claim-frequency/freMTPL2freq.html#model-without-area",
    "title": "Modelling the frequency of insurance claims",
    "section": "4.3 Model without Area",
    "text": "4.3 Model without Area\n\nglm_2 = glm(\n    @formula(\n        ClaimNb ~ VehPower + VehAge + DrivAge + BonusMalus \n                + VehBrand + VehGas + log(Density) + Region), \n        df_train, Poisson(), LogLink(); \n        offset=df_train.Exposure\n)\n\nStatsModels.TableRegressionModel{GeneralizedLinearModel{GLM.GlmResp{Vector{Float64}, Poisson{Float64}, LogLink}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\nClaimNb ~ 1 + VehPower + VehAge + DrivAge + BonusMalus + VehBrand + VehGas + :(log(Density)) + Region\n\nCoefficients:\n─────────────────────────────────────────────────────────────────────────────────────\n                       Coef.   Std. Error       z  Pr(&gt;|z|)    Lower 95%    Upper 95%\n─────────────────────────────────────────────────────────────────────────────────────\n(Intercept)      -5.31737     0.0561142    -94.76    &lt;1e-99  -5.42736     -5.20739\nVehPower          0.00886097  0.00307154     2.88    0.0039   0.00284086   0.0148811\nVehAge           -0.036323    0.00130322   -27.87    &lt;1e-99  -0.0388773   -0.0337687\nDrivAge           0.00916437  0.000443934   20.64    &lt;1e-93   0.00829428   0.0100345\nBonusMalus        0.020271    0.000351977   57.59    &lt;1e-99   0.0195811    0.0209608\nVehBrand: B10    -0.00988575  0.04116       -0.24    0.8102  -0.0905579    0.0707864\nVehBrand: B11     0.0322316   0.0446473      0.72    0.4703  -0.0552755    0.119739\nVehBrand: B12     0.0593091   0.0196046      3.03    0.0025   0.0208848    0.0977335\nVehBrand: B13     0.038053    0.0457224      0.83    0.4053  -0.0515612    0.127667\nVehBrand: B14    -0.113816    0.0863137     -1.32    0.1873  -0.282988     0.0553558\nVehBrand: B2      0.00482425  0.0171325      0.28    0.7783  -0.0287548    0.0384033\nVehBrand: B3     -0.00120893  0.0245144     -0.05    0.9607  -0.0492562    0.0468384\nVehBrand: B4     -0.0103655   0.0329763     -0.31    0.7533  -0.0749979    0.0542669\nVehBrand: B5      0.103011    0.0275753      3.74    0.0002   0.0489647    0.157058\nVehBrand: B6     -0.0521469   0.0318823     -1.64    0.1019  -0.114635     0.0103413\nVehGas: Regular   0.0788155   0.0122417      6.44    &lt;1e-09   0.0548222    0.102809\nlog(Density)      0.0322299   0.00379182     8.50    &lt;1e-16   0.0247981    0.0396618\nRegion: R21       0.174542    0.0882277      1.98    0.0479   0.00161863   0.347465\nRegion: R22       0.0856019   0.057073       1.50    0.1336  -0.0262592    0.197463\nRegion: R23      -0.19873     0.0673562     -2.95    0.0032  -0.330746    -0.0667144\nRegion: R24       0.166623    0.0260824      6.39    &lt;1e-09   0.115502     0.217743\nRegion: R25       0.133309    0.0487089      2.74    0.0062   0.0378412    0.228776\nRegion: R26      -0.0100497   0.0541379     -0.19    0.8527  -0.116158     0.0960587\nRegion: R31      -0.117859    0.0377951     -3.12    0.0018  -0.191936    -0.0437816\nRegion: R41      -0.122233    0.048979      -2.50    0.0126  -0.21823     -0.0262358\nRegion: R42       0.137304    0.0959349      1.43    0.1524  -0.0507247    0.325333\nRegion: R43      -0.129647    0.152135      -0.85    0.3941  -0.427826     0.168531\nRegion: R52       0.0756915   0.0323915      2.34    0.0195   0.0122054    0.139178\nRegion: R53       0.226256    0.0303665      7.45    &lt;1e-13   0.166739     0.285773\nRegion: R54       0.00442079  0.0426999      0.10    0.9175  -0.0792696    0.0881111\nRegion: R72      -0.0682786   0.0362678     -1.88    0.0598  -0.139362     0.00280495\nRegion: R73      -0.136858    0.0470298     -2.91    0.0036  -0.229035    -0.0446816\nRegion: R74       0.153509    0.0761688      2.02    0.0439   0.00422137   0.302798\nRegion: R82       0.134816    0.0251175      5.37    &lt;1e-07   0.0855862    0.184045\nRegion: R83      -0.301865    0.0847293     -3.56    0.0004  -0.467931    -0.135799\nRegion: R91      -0.0743314   0.0351531     -2.11    0.0345  -0.14323     -0.00543257\nRegion: R93       0.0204767   0.0260334      0.79    0.4315  -0.0305479    0.0715012\nRegion: R94       0.0901658   0.0750487      1.20    0.2296  -0.056927     0.237259\n─────────────────────────────────────────────────────────────────────────────────────\n\n\nFrom the coefficient table above we observe that now \\(\\log\\)(density) is a significant feature, as indicated by the small \\(p\\) and large \\(z\\) value. Lets compare the Poisson loss, MAE and MSE of the model with all features but the Area to the other models:\n\ncalculate_losses([glm_1, glm_2], [\"all_features\", \"without area\"])\n\n3×7 DataFrame\n\n\n\nRow\nModels\nTrain_PoissonLoss\nTrain_L1DistLoss\nTrain_L2DistLoss\nTest_PoissonLoss\nTest_L1DistLoss\nTest_L2DistLoss\n\n\n\nString\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nBase\n0.31513\n0.10035\n0.0562994\n0.316145\n0.100531\n0.0566815\n\n\n2\nall features\n0.307316\n0.0994627\n0.0557037\n0.308429\n0.0997102\n0.0560861\n\n\n3\nwithout area\n0.307321\n0.0994629\n0.0557041\n0.308402\n0.0997072\n0.0560835\n\n\n\n\n\n\nWe observe that the model with all features but the Area has nearly the same losses and errors as the model with all features. This is not surprising, as the Area feature was not significant in the model with all features.\nInvestigating the coefficient table of the glm_2 model reveals that some of the categorical features appear non-significant. A further refinenment of the model would be to remove some of the least significant features. However, this is left as a future task."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "gorans blog",
    "section": "",
    "text": "Regularisation in Lux.jl\n\n\n\n\n\n\nJulia\n\n\nML\n\n\n\nHow to use regularisation in Lux.jl with Optimisers.jl\n\n\n\n\n\nFeb 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nModelling the frequency of insurance claims\n\n\n\n\n\n\nJulia\n\n\ndatascience\n\n\n\nExploration of the freMTPL2freq dataset with Julia\n\n\n\n\n\nDec 30, 2024\n\n\n\n\n\n\nNo matching items"
  }
]