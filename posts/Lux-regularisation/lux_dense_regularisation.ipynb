{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Regularisation in `Lux.jl`\n",
    "date: 2025-02-10\n",
    "description: How to use regularisation in `Lux.jl` with `Optimisers.jl`\n",
    "categories: [Julia, ML]\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-expand: 1\n",
    "    number-sections: true\n",
    "    number-depth: 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is a common technique in machine learning used to improve model performance and generalization. It involves adding a penalty term to the loss function, typically to prevent overfitting. Overfitting occurs when a model captures patterns specific to the training data but fails to generalize well to unseen data. Regularization can also help control exploding weights or enforce sparsity, which can lead to more interpretable models. In this notebook, we will demonstrate how to incorporate regularization into model training using `Lux.jl`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Lux.jl` is a machine learning library built entirely in pure Julia. It is designed for simplicity, flexibility, and high performance. One of its defining features is the strict separation between model parameters and layer structures. This approach is somewhat different from other machine learning libraries. But it provides deeper insight into the inner workings of model training and architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first install `Lux.jl` and other required packages ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.activate(\".\")\n",
    "Pkg.add([\"Lux\", \"Random\", \"Printf\", \"Enzyme\", \"Optimisers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and load them into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Lux, Random, Printf, Enzyme, Optimisers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides `Lux` we need \n",
    "- `Random` for generating random numbers\n",
    "- `Printf` for formatted printing\n",
    "- `Enzyme` for automatic differentiation of the models and loss functions\n",
    "- `Optimisers` for the ADAM optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define a simple multi-layer perceptron (MLP) model with a single hidden layer and $\\tanh$ activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "    layer_1 = Dense(2 => 4, tanh),      \u001b[90m# 12 parameters\u001b[39m\n",
       "    layer_2 = Dense(4 => 1),            \u001b[90m# 5 parameters\u001b[39m\n",
       ") \u001b[90m        # Total: \u001b[39m17 parameters,\n",
       "\u001b[90m          #        plus \u001b[39m0 states."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Chain(\n",
    "    Dense(2, 4, tanh),\n",
    "    Dense(4, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the separation of model parameters and layer structure, `Lux` takes randomness very seriously, too. To align with this design philosophy, we make this notebook reproducible by fixing the seed of the random number generator and generating dummy training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×100 Matrix{Float32}:\n",
       " -1.48774  -1.55746  -0.013772  0.423002  …  -0.749528  -0.293559  -1.15156"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rng = Random.default_rng()\n",
    "Random.seed!(rng, 42)\n",
    "\n",
    "X_train = randn(rng, Float32, 2,100)\n",
    "y_train = randn(rng, Float32, 1,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, `Lux` keeps model parameters and layer structures separate. The parameters are set up using a `setup` method, which returns the parameters `ps` and the layer states `st`. In our simple MLP example, the layers don’t have any states. But for layers like batch normalization, you’d see states being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((layer_1 = (weight = Float32[0.353489 -1.8094461; 1.5090263 -1.1654156; -0.83640474 -1.1235346; 1.8991852 1.449335], bias = Float32[0.41897145, -0.069400184, -0.6758513, 0.67588615]), layer_2 = (weight = Float32[0.44184172 0.014959536 0.5158945 0.3650591], bias = Float32[0.24236047])), (layer_1 = NamedTuple(), layer_2 = NamedTuple()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ps, st = LuxCore.setup(rng, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, we simply call it like a regular function, passing the model parameters (and the empty states) as function arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[0.7788858], (layer_1 = NamedTuple(), layer_2 = NamedTuple()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model(X_train[:,1], ps, st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to regularize a model. Some common techniques include $l_1$ and $l_2$ regularization. In this notebook, we will focus on L2 regularization, which is also known as weight decay. It involves adding a penalty term to the loss function that is proportional to the square of the model parameters (weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my first attempt to implement $l_2$ regularisation of model parameters I wrote the following custom loss function:\n",
    "\n",
    "```{julia}\n",
    "function loss_function(model, ps, st, (x, y))\n",
    "    T = eltype(Base.Flatten(first(ps)))\n",
    "\n",
    "    loss_mse = MSELoss()(model, ps, st, (x,y))[1]\n",
    "\n",
    "    loss_reg = zero(T)\n",
    "    for p in ps\n",
    "        loss_reg += sum(abs2, Base.Flatten(p))\n",
    "    end\n",
    "\n",
    "    loss_total = loss_mse + convert(T, 0.001) * loss_reg\n",
    "\n",
    "    return loss_total, st, NamedTuple()\n",
    "end\n",
    "```\n",
    "\n",
    "It works fine but isn't the most extendable solution. The Julia community on [Discourse](https://discourse.julialang.org/t/custom-loss-functions-in-lux-jl/125661) suggested using the `WeightDecay` function from `Optimisers.jl`, which does exactly what we need. By chaining it with the `MSELoss` function, we can train our dummy MLP model with $l_2$ regularization like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0001 \t Loss: 1.24611223\n",
      "Iteration: 0101 \t Loss: 0.991424084\n",
      "Iteration: 0201 \t Loss: 0.959512353\n",
      "Iteration: 0301 \t Loss: 0.949317396\n",
      "Iteration: 0401 \t Loss: 0.941184938\n",
      "Iteration: 0501 \t Loss: 0.932920218\n",
      "Iteration: 0601 \t Loss: 0.922357559\n",
      "Iteration: 0701 \t Loss: 0.910138071\n",
      "Iteration: 0801 \t Loss: 0.898681343\n",
      "Iteration: 0901 \t Loss: 0.888456821\n",
      "Iteration: 1000 \t Loss: 0.879557967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Chain{@NamedTuple{layer_1::Dense{typeof(tanh), Int64, Int64, Nothing, Nothing, Static.True}, layer_2::Dense{typeof(identity), Int64, Int64, Nothing, Nothing, Static.True}}, Nothing}((layer_1 = Dense(2 => 4, tanh), layer_2 = Dense(4 => 1)), nothing), (layer_1 = (weight = Float32[5.620661 -4.1725636; 3.3105361 -3.3534684; -1.4295509 -0.7033358; 3.2561698 1.6087654], bias = Float32[0.7682078, 0.48789456, -0.012730246, -0.3011559]), layer_2 = (weight = Float32[1.2095982 -1.1726627 1.2155563 0.9848269], bias = Float32[0.29594985])), (layer_1 = NamedTuple(), layer_2 = NamedTuple()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# code adapted from the Lux documentation https://lux.csail.mit.edu/stable/\n",
    "function train_model!(model, ps, st, x, y)\n",
    "\n",
    "    train_state = Lux.Training.TrainState(model, ps, st,\n",
    "        # here we chain together the optimiser Adam with \n",
    "        # a WeightDecay of 0.001. \n",
    "        OptimiserChain(Adam(0.01f0), WeightDecay(0.001)),\n",
    "    )\n",
    "\n",
    "    for iter in 1:1000\n",
    "        _, loss, _, train_state = Lux.Training.single_train_step!(\n",
    "            AutoEnzyme(),\n",
    "            MSELoss(),\n",
    "            (x, y), train_state\n",
    "        )\n",
    "        if iter % 100 == 1 || iter == 1000\n",
    "            @printf \"Iteration: %04d \\t Loss: %10.9g\\n\" iter loss\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return model, ps, st\n",
    "end\n",
    "\n",
    "train_model!(model, ps, st, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Just chain together the optimizer `Adam` with `WeightDecay` via `OptimiserChain` and we have $l_2$ regularisation of model weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
